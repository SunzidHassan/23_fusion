<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
<!--   <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>-->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630 -->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


<!--   <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">-->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
<!--   <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">-->
  <!-- Keywords for your paper to be indexed by-->  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multi-modal Robotic Odor Source Localization">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Robotic Odor Source Localization via Vision and Olfaction Fusion Navigation Algorithm</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Academic Project Page</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Sunzid Hassan</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Lingxiao Wang</a>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Khan Raqib Mahmud</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Louisiana Tech University<br>MDPI Sensors, 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Sensors paper link -->
                      <span class="link-block">
                        <a href="https://www.mdpi.com/2740946" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/SunzidHassan/23_fusion_cp" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                <!-- </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Fusion.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Sample osl navigation run using the vision and olfaction fusion navigation algorithm. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Odor Source Localization (OSL) technology allows autonomous agents like mobile robots to find an unknown odor source in a given environment. An effective navigation algorithm that guides the robot to approach the odor source is the key to successfully locating the odor source. Downside of traditional olfaction-only OSL methods is that they struggle to localize odor sources in real-world environments with complex airflow. Our proposed solution integrates vision and olfaction sensor modalities to localize odor sources even if olfaction sensing is disrupted by turbulent airflow or vision sensing is impaired by environmental complexities. The model leverages the zero-shot multi-modal reasoning capabilities of large language models (LLMs), negating the requirement of manual knowledge encoding or custom-trained supervised learning models. A key feature of the proposed algorithm is the `High-level Reasoning' module, which encodes the olfaction and vision sensor data into a multi-modal prompt and instructs the LLM to employ a hierarchical reasoning process to select an appropriate high-level navigation behavior. Subsequently, the `Low-level Action' module translates the selected high-level navigation behavior into low-level action commands that can be executed by the mobile robot. To validate our method, we implemented the proposed algorithm on a mobile robot in a complex, real-world search environment that presents challenges to both olfaction and vision-sensing modalities. We compared the performance of our proposed algorithm to single sensory modality-based olfaction-only and vision-only navigation algorithms, and a supervised learning-based vision and olfaction fusion navigation algorithm. Experimental results demonstrate that multi-sensory navigation algorithms are statistically superior to single sensory navigation algorithms. The proposed algorithm outperformed the other algorithms in both laminar and turbulent airflow environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Methodology -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <figure>
            <img src="static\images\FusionFlowDiagram.png" alt="Flow diagram" style="max-width: 60%;">
            <figcaption>Figure: Flow diagram of the Proposed Vision and Olfaction Fusion Navigation Algorithm.</figcaption>
          </figure>
          <p>
            We trained a YOLOv7 model to detect visible odor plumes. If the model detects a visible odor plume, the navigation algorithm follows 'Vision-based Navigation'. Otherwise, it follows 'Olfaction-based Navigation' to localize the odor source.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Methodology -->


<!-- Paper Experiment -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment</h2>
        <div class="content has-text-justified">
          <figure>
            <img src="static\images\SearchArea.png" alt="Search Area" style="max-width: 100%;">
            <figcaption>Figure: Search area used in experiments.</figcaption>
          </figure>
          <p>
            Search area: The focus of the experiment is to test if the proposed navigation algorithm can avoid the obstacle and navigate to the odor source in laminar and turbulent airflow environments.
          </p>
          <p>
            To determine the effectiveness of the multimodal fusion navigation algorithm, we compared the proposed algorithm to single sensory modality-based 'Olfaction-only' and 'Vision-only' navigation algorithms.
          </p>
          <figure>
            <img src="static\images\robotPlatform.png" alt="Robot Platform" style="max-width: 100%;">
            <figcaption>Figure: Robotic platform used in experiments.</figcaption>
          </figure>
          <p>
            The robotic platform used for this task utilizes a Raspberry Pi Camera for vision sensing, an MQ3 alcohol detector and a WindSonic Anemometer for olfaction sensing, and a LDS-02 Laser Distance Sensor.
          </p>
          <figure>
            <img src="static\images\sampleRun.png" alt="Sample Run" style="max-width: 100%;">
            <figcaption>Figure: Sample test results.</figcaption>
          </figure>
          <figure>
            <img src="static\images\Results.png" alt="Experiment Results" style="max-width: 100%;">
            <figcaption>Figure: Repeated test results.</figcaption>
          </figure>
          <p>
            The results show that the proposed Fusion navigation algorithm (e1vo and e2vo) outperformed both the olfaction-only (e1o and e2o) and vision-only (e1v and e2v) navigation algorithms. The performance gap was greater in turbulent airflow environment (e2).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Experiment -->





<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sensors-24-02309-v2.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{hassan2024robotic,
        title={Robotic Odor Source Localization via Vision and Olfaction Fusion Navigation Algorithm},
        author={Hassan, Sunzid and Wang, Lingxiao and Mahmud, Khan Raqib},
        journal={Sensors},
        volume={24},
        number={7},
        pages={2309},
        year={2024},
        publisher={MDPI}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
